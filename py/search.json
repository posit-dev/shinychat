[
  {
    "objectID": "api/playwright.ChatController.html",
    "href": "api/playwright.ChatController.html",
    "title": "playwright.ChatController",
    "section": "",
    "text": "playwright.ChatController(page, id)\nController for :func:shiny.ui.chat.\n\n\n\n\n\nName\nDescription\n\n\n\n\nloc\nPlaywright Locator for the chat.\n\n\nloc_input\nPlaywright Locator for the chat’s \n\n\nloc_input_button\nPlaywright Locator for the chat’s  input.\n\n\nloc_input_container\nPlaywright Locator for the chat input container.\n\n\nloc_latest_message\nPlaywright Locator for the last message in the chat.\n\n\nloc_messages\nPlaywright Locator for the chat messages.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nexpect_latest_message\nExpects the last message in the chat.\n\n\nexpect_messages\nExpects the chat messages.\n\n\nexpect_user_input\nExpects the user message in the chat.\n\n\nsend_user_input\nSends the user message in the chat.\n\n\nset_user_input\nSets the user message in the chat.\n\n\n\n\n\nplaywright.ChatController.expect_latest_message(value, *, timeout=None)\nExpects the last message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nPatternOrStr\nThe expected last message.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the expectation to pass. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.expect_messages(value, *, timeout=None)\nExpects the chat messages.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nPatternOrStr\nThe expected messages.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the expectation to pass. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.expect_user_input(value, *, timeout=None)\nExpects the user message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nPatternOrStr\nThe expected user message.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the expectation to pass. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.send_user_input(method='enter', timeout=None)\nSends the user message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\nLiteral['enter', 'click']\nThe method to send the user message. Defaults to \"enter\".\n'enter'\n\n\ntimeout\nTimeout\nThe maximum time to wait for the chat input to be visible and interactable. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.set_user_input(value, *, timeout=None)\nSets the user message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nstr\nThe message to send.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the chat input to be visible and interactable. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Testing",
      "playwright.ChatController"
    ]
  },
  {
    "objectID": "api/playwright.ChatController.html#attributes",
    "href": "api/playwright.ChatController.html#attributes",
    "title": "playwright.ChatController",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nloc\nPlaywright Locator for the chat.\n\n\nloc_input\nPlaywright Locator for the chat’s \n\n\nloc_input_button\nPlaywright Locator for the chat’s  input.\n\n\nloc_input_container\nPlaywright Locator for the chat input container.\n\n\nloc_latest_message\nPlaywright Locator for the last message in the chat.\n\n\nloc_messages\nPlaywright Locator for the chat messages.",
    "crumbs": [
      "API Reference",
      "Testing",
      "playwright.ChatController"
    ]
  },
  {
    "objectID": "api/playwright.ChatController.html#methods",
    "href": "api/playwright.ChatController.html#methods",
    "title": "playwright.ChatController",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nexpect_latest_message\nExpects the last message in the chat.\n\n\nexpect_messages\nExpects the chat messages.\n\n\nexpect_user_input\nExpects the user message in the chat.\n\n\nsend_user_input\nSends the user message in the chat.\n\n\nset_user_input\nSets the user message in the chat.\n\n\n\n\n\nplaywright.ChatController.expect_latest_message(value, *, timeout=None)\nExpects the last message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nPatternOrStr\nThe expected last message.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the expectation to pass. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.expect_messages(value, *, timeout=None)\nExpects the chat messages.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nPatternOrStr\nThe expected messages.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the expectation to pass. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.expect_user_input(value, *, timeout=None)\nExpects the user message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nPatternOrStr\nThe expected user message.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the expectation to pass. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.send_user_input(method='enter', timeout=None)\nSends the user message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\nLiteral['enter', 'click']\nThe method to send the user message. Defaults to \"enter\".\n'enter'\n\n\ntimeout\nTimeout\nThe maximum time to wait for the chat input to be visible and interactable. Defaults to None.\nNone\n\n\n\n\n\n\n\nplaywright.ChatController.set_user_input(value, *, timeout=None)\nSets the user message in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nstr\nThe message to send.\nrequired\n\n\ntimeout\nTimeout\nThe maximum time to wait for the chat input to be visible and interactable. Defaults to None.\nNone",
    "crumbs": [
      "API Reference",
      "Testing",
      "playwright.ChatController"
    ]
  },
  {
    "objectID": "api/express.Chat.html",
    "href": "api/express.Chat.html",
    "title": "express.Chat",
    "section": "",
    "text": "express.Chat(id, *, messages=(), on_error='auto', tokenizer=None)\n\n\n\n\n\nName\nDescription\n\n\n\n\nlatest_message_stream\nReact to changes in the latest message stream.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nappend_message\nAppend a message to the chat.\n\n\nappend_message_stream\nAppend a message as a stream of message chunks.\n\n\nclear_messages\nClear all chat messages.\n\n\ndestroy\nDestroy the chat instance.\n\n\nenable_bookmarking\nEnable bookmarking for the chat instance.\n\n\nmessage_stream_context\nMessage stream context manager.\n\n\nmessages\nReactively read chat messages\n\n\non_user_submit\nDefine a function to invoke when user input is submitted.\n\n\nset_user_message\nDeprecated. Use update_user_input(value=value) instead.\n\n\ntransform_assistant_response\nTransform assistant responses.\n\n\ntransform_user_input\nTransform user input.\n\n\nui\nCreate a UI element for this Chat.\n\n\nupdate_user_input\nUpdate the user input.\n\n\nuser_input\nReactively read the user’s message.\n\n\n\n\n\nexpress.Chat.append_message(message, *, icon=None)\nAppend a message to the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nAny\nA given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | TagList | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nInput suggestions\n\n\n\nInput suggestions are special links that send text to the user input box when clicked (or accessed via keyboard). They can be created in the following ways:\n\n&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;: An image link with the same functionality as above.\n&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\nAdding a submit CSS class or a data-suggestion-submit=\"true\" attribute to the suggestion element.\nHolding the Ctrl/Cmd key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the Alt/Option key while clicking the suggestion link.\n\n\n\n\n\n\n\n\nStreamed messages\n\n\n\nUse .append_message_stream() instead of this method when stream=True (or similar) is specified in model’s completion method.\n\n\n\n\n\n\nexpress.Chat.append_message_stream(message, *, icon=None)\nAppend a message as a stream of message chunks.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nIterable[Any] | AsyncIterable[Any]\nAn (async) iterable of message chunks. Each chunk can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\nInput suggestions are special links that send text to the user input box when\nclicked (or accessed via keyboard). They can be created in the following ways:\n\n* `&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;`: An inline text link that\n    places 'Suggestion text' in the user input box when clicked.\n* `&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;`: An image link with\n    the same functionality as above.\n* `&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;`: An inline text\n    link that places 'Suggestion text' in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\n* Adding a `submit` CSS class or a `data-suggestion-submit=\"true\"` attribute to\n  the suggestion element.\n* Holding the `Ctrl/Cmd` key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the\n`Alt/Option` key while clicking the suggestion link.\nUse this method (over `.append_message()`) when `stream=True` (or similar) is\nspecified in model's completion method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nAn extended task that represents the streaming task. The .result() method of the task can be called in a reactive context to get the final state of the stream.\n\n\n\n\n\n\n\nexpress.Chat.clear_messages()\nClear all chat messages.\n\n\n\nexpress.Chat.destroy()\nDestroy the chat instance.\n\n\n\nexpress.Chat.enable_bookmarking(\n    client,\n    /,\n    *,\n    bookmark_store=None,\n    bookmark_on='response',\n)\nEnable bookmarking for the chat instance.\nThis method registers on_bookmark and on_restore hooks on session.bookmark (:class:shiny.bookmark.Bookmark) to save/restore chat state on both the Chat and client= instances. In order for this method to actually work correctly, a bookmark_store= must be specified in shiny.express.app_opts().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nclient\nClientWithState | chatlas.Chat[Any, Any]\nThe chat client instance to use for bookmarking. This can be a Chat model provider from chatlas, or more generally, an instance following the ClientWithState protocol.\nrequired\n\n\nbookmark_store\nOptional[BookmarkStore]\nA convenience parameter to set the shiny.express.app_opts(bookmark_store=) which is required for bookmarking (and .enable_bookmarking()). If None, no value will be set.\nNone\n\n\nbookmark_on\nOptional[Literal['response']]\nThe event to trigger the bookmarking on. Supported values include: - \"response\" (the default): a bookmark is triggered when the assistant is done responding. - None: no bookmark is triggered When this method triggers a bookmark, it also updates the URL query string to reflect the bookmarked state.\n'response'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the Shiny App does have bookmarking enabled.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nCancelCallback\nA callback to cancel the bookmarking hooks.\n\n\n\n\n\n\n\nexpress.Chat.message_stream_context()\n    Message stream context manager.\n\n    A context manager for appending streaming messages into the chat. This context\n    manager can:\n\n    1. Be used in isolation to append a new streaming message to the chat.\n        * Compared to `.append_message_stream()` this method is more flexible but\n          isn't non-blocking by default (i.e., it doesn't launch an extended task).\n    2. Be nested within itself\n        * Nesting is primarily useful for making checkpoints to `.clear()` back\n          to (see the example below).\n    3. Be used from within a `.append_message_stream()`\n        * Useful for inserting additional content from another context into the\n          stream (e.g., see the note about tool calls below).\n\n\n    :\n        A `MessageStream` class instance, which has a method for `.append()`ing\n        message content chunks to as well as way to `.clear()` the stream back to\n        it's initial state. Note that `.append()` supports the same message content\n        types as `.append_message()`.\n\n\n\n    ```python\n    import asyncio\n\n    from shiny import reactive\n    from shiny.express import ui\n\n    chat = ui.Chat(id=\"my_chat\")\n    chat.ui()\n\n\n    @reactive.effect\n    async def _():\n        async with chat.message_stream_context() as msg:\n            await msg.append(\"Starting stream...\nProgress:“) async with chat.message_stream_context() as progress: for x in [0, 50, 100]: await progress.append(f” {x}%“) await asyncio.sleep(1) await progress.clear() await msg.clear() await msg.append(”Completed stream”) ```\n\n\n\n    A useful pattern for displaying tool calls in a chatbot is for the tool to\n    display using `.message_stream_context()` while the the response generation is\n    happening through `.append_message_stream()`. This allows the tool to display\n    things like progress updates (or other \"ephemeral\" content) and optionally\n    `.clear()` the stream back to it's initial state when ready to display the\n    \"final\" content.\n\n\n\n\nexpress.Chat.messages(\n    format=MISSING,\n    token_limits=None,\n    transform_user='all',\n    transform_assistant=False,\n)\nReactively read chat messages\nObtain chat messages within a reactive context. The default behavior is intended for passing messages along to a model for response generation where you typically want to:\n\nCap the number of tokens sent in a single request (i.e., token_limits).\nApply user input transformations (i.e., transform_user), if any.\nNot apply assistant response transformations (i.e., transform_assistant) since these are predominantly for display purposes (i.e., the model shouldn’t concern itself with how the responses are displayed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat\nMISSING_TYPE | ProviderMessageFormat\nThe message format to return. The default value of MISSING means chat messages are returned as :class:ChatMessage objects (a dictionary with content and role keys). Other supported formats include: * \"anthropic\": Anthropic message format. * \"google\": Google message (aka content) format. * \"langchain\": LangChain message format. * \"openai\": OpenAI message format. * \"ollama\": Ollama message format.\nMISSING\n\n\ntoken_limits\ntuple[int, int] | None\nLimit the conversation history based on token limits. If specified, only the most recent messages that fit within the token limits are returned. This is useful for avoiding “exceeded token limit” errors when sending messages to the relevant model, while still providing the most recent context available. A specified value must be a tuple of two integers. The first integer is the maximum number of tokens that can be sent to the model in a single request. The second integer is the amount of tokens to reserve for the model’s response. Note that token counts based on the tokenizer provided to the Chat constructor.\nNone\n\n\ntransform_user\nLiteral['all', 'last', 'none']\nWhether to return user input messages with transformation applied. This only matters if a transform_user_input was provided to the chat constructor. The default value of \"all\" means all user input messages are transformed. The value of \"last\" means only the last user input message is transformed. The value of \"none\" means no user input messages are transformed.\n'all'\n\n\ntransform_assistant\nbool\nWhether to return assistant messages with transformation applied. This only matters if an transform_assistant_response was provided to the chat constructor.\nFalse\n\n\n\n\n\n\nMessages are listed in the order they were added. As a result, when this method is called in a .on_user_submit() callback (as it most often is), the last message will be the most recent one submitted by the user.\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[ChatMessage, …]\nA tuple of chat messages.\n\n\n\n\n\n\n\nexpress.Chat.on_user_submit(fn=None)\nDefine a function to invoke when user input is submitted.\nApply this method as a decorator to a function (fn) that should be invoked when the user submits a message. This function can take an optional argument, which will be the user input message.\nIn many cases, the implementation of fn should also do the following:\n\nGenerate a response based on the user input.\n\n\nIf the response should be aware of chat history, use a package like chatlas to manage the chat state, or use the .messages() method to get the chat history.\n\n\nAppend that response to the chat component using .append_message() ( or .append_message_stream() if the response is streamed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nUserSubmitFunction | None\nA function to invoke when user input is submitted.\nNone\n\n\n\n\n\n\nThis method creates a reactive effect that only gets invalidated when the user submits a message. Thus, the function fn can read other reactive dependencies, but it will only be re-invoked when the user submits a message.\n\n\n\n\nexpress.Chat.set_user_message(value)\nDeprecated. Use update_user_input(value=value) instead.\n\n\n\nexpress.Chat.transform_assistant_response(fn=None)\nTransform assistant responses.\nUse this method as a decorator on a function (fn) that transforms assistant responses before displaying them in the chat. This is useful for post-processing model responses before displaying them to the user.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformAssistantResponseFunction | None\nA function that takes a string and returns either a string, :class:shiny.ui.HTML, or None. If fn returns a string, it gets interpreted and parsed as a markdown on the client (and the resulting HTML is then sanitized). If fn returns :class:shiny.ui.HTML, it will be displayed as-is. If fn returns None, the response is effectively ignored.\nNone\n\n\n\n\n\n\nWhen doing an .append_message_stream(), fn gets called on every chunk of the response (thus, it should be performant), and can optionally access more information (i.e., arguments) about the stream. The 1st argument (required) contains the accumulated content, the 2nd argument (optional) contains the current chunk, and the 3rd argument (optional) is a boolean indicating whether this chunk is the last one in the stream.\n\n\n\n\nexpress.Chat.transform_user_input(fn=None)\nTransform user input.\nUse this method as a decorator on a function (fn) that transforms user input before storing it in the chat messages returned by .messages(). This is useful for implementing RAG workflows, like taking a URL and scraping it for text before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformUserInput | TransformUserInputAsync | None\nA function to transform user input before storing it in the chat .messages(). If fn returns None, the user input is effectively ignored, and .on_user_submit() callbacks are suspended until more input is submitted. This behavior is often useful to catch and handle errors that occur during transformation. In this case, the transform function should append an error message to the chat (via .append_message()) to inform the user of the error.\nNone\n\n\n\n\n\n\n\nexpress.Chat.ui(\n    messages=None,\n    placeholder='Enter a message...',\n    width='min(680px, 100%)',\n    height='auto',\n    fill=True,\n    icon_assistant=None,\n    **kwargs,\n)\nCreate a UI element for this Chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessages\nOptional[Sequence[TagChild | ChatMessageDict]]\nA sequence of messages to display in the chat. Each message can be either a string or a dictionary with content and role keys. The content key should contain the message text, and the role key can be “assistant” or “user”.\nNone\n\n\nplaceholder\nstr\nPlaceholder text for the chat input.\n'Enter a message...'\n\n\nwidth\nCssUnit\nThe width of the UI element.\n'min(680px, 100%)'\n\n\nheight\nCssUnit\nThe height of the UI element.\n'auto'\n\n\nfill\nbool\nWhether the chat should vertically take available space inside a fillable container.\nTrue\n\n\nicon_assistant\nHTML | Tag | TagList | None\nThe icon to use for the assistant chat messages. Can be a HTML or a tag in the form of :class:~htmltools.HTML or :class:~htmltools.Tag. If None, a default robot icon is used.\nNone\n\n\nkwargs\nTagAttrValue\nAdditional attributes for the chat container element.\n{}\n\n\n\n\n\n\n\nexpress.Chat.update_user_input(\n    value=None,\n    placeholder=None,\n    submit=False,\n    focus=False,\n)\nUpdate the user input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nstr | None\nThe value to set the user input to.\nNone\n\n\nplaceholder\nstr | None\nThe placeholder text for the user input.\nNone\n\n\nsubmit\nbool\nWhether to automatically submit the text for the user. Requires value.\nFalse\n\n\nfocus\nbool\nWhether to move focus to the input element. Requires value.\nFalse\n\n\n\n\n\n\n\nexpress.Chat.user_input(transform=False)\nReactively read the user’s message.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntransform\nbool\nWhether to apply the user input transformation function (if one was provided).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr | None\nThe user input message (before any transformation).\n\n\n\n\n\n\nMost users shouldn’t need to use this method directly since the last item in .messages() contains the most recent user input. It can be useful for:\n\nTaking a reactive dependency on the user’s input outside of a .on_user_submit() callback.\nMaintaining message state separately from .messages().",
    "crumbs": [
      "API Reference",
      "Shiny Express",
      "express.Chat"
    ]
  },
  {
    "objectID": "api/express.Chat.html#attributes",
    "href": "api/express.Chat.html#attributes",
    "title": "express.Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlatest_message_stream\nReact to changes in the latest message stream.",
    "crumbs": [
      "API Reference",
      "Shiny Express",
      "express.Chat"
    ]
  },
  {
    "objectID": "api/express.Chat.html#methods",
    "href": "api/express.Chat.html#methods",
    "title": "express.Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nappend_message\nAppend a message to the chat.\n\n\nappend_message_stream\nAppend a message as a stream of message chunks.\n\n\nclear_messages\nClear all chat messages.\n\n\ndestroy\nDestroy the chat instance.\n\n\nenable_bookmarking\nEnable bookmarking for the chat instance.\n\n\nmessage_stream_context\nMessage stream context manager.\n\n\nmessages\nReactively read chat messages\n\n\non_user_submit\nDefine a function to invoke when user input is submitted.\n\n\nset_user_message\nDeprecated. Use update_user_input(value=value) instead.\n\n\ntransform_assistant_response\nTransform assistant responses.\n\n\ntransform_user_input\nTransform user input.\n\n\nui\nCreate a UI element for this Chat.\n\n\nupdate_user_input\nUpdate the user input.\n\n\nuser_input\nReactively read the user’s message.\n\n\n\n\n\nexpress.Chat.append_message(message, *, icon=None)\nAppend a message to the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nAny\nA given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | TagList | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nInput suggestions\n\n\n\nInput suggestions are special links that send text to the user input box when clicked (or accessed via keyboard). They can be created in the following ways:\n\n&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;: An image link with the same functionality as above.\n&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\nAdding a submit CSS class or a data-suggestion-submit=\"true\" attribute to the suggestion element.\nHolding the Ctrl/Cmd key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the Alt/Option key while clicking the suggestion link.\n\n\n\n\n\n\n\n\nStreamed messages\n\n\n\nUse .append_message_stream() instead of this method when stream=True (or similar) is specified in model’s completion method.\n\n\n\n\n\n\nexpress.Chat.append_message_stream(message, *, icon=None)\nAppend a message as a stream of message chunks.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nIterable[Any] | AsyncIterable[Any]\nAn (async) iterable of message chunks. Each chunk can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\nInput suggestions are special links that send text to the user input box when\nclicked (or accessed via keyboard). They can be created in the following ways:\n\n* `&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;`: An inline text link that\n    places 'Suggestion text' in the user input box when clicked.\n* `&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;`: An image link with\n    the same functionality as above.\n* `&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;`: An inline text\n    link that places 'Suggestion text' in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\n* Adding a `submit` CSS class or a `data-suggestion-submit=\"true\"` attribute to\n  the suggestion element.\n* Holding the `Ctrl/Cmd` key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the\n`Alt/Option` key while clicking the suggestion link.\nUse this method (over `.append_message()`) when `stream=True` (or similar) is\nspecified in model's completion method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nAn extended task that represents the streaming task. The .result() method of the task can be called in a reactive context to get the final state of the stream.\n\n\n\n\n\n\n\nexpress.Chat.clear_messages()\nClear all chat messages.\n\n\n\nexpress.Chat.destroy()\nDestroy the chat instance.\n\n\n\nexpress.Chat.enable_bookmarking(\n    client,\n    /,\n    *,\n    bookmark_store=None,\n    bookmark_on='response',\n)\nEnable bookmarking for the chat instance.\nThis method registers on_bookmark and on_restore hooks on session.bookmark (:class:shiny.bookmark.Bookmark) to save/restore chat state on both the Chat and client= instances. In order for this method to actually work correctly, a bookmark_store= must be specified in shiny.express.app_opts().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nclient\nClientWithState | chatlas.Chat[Any, Any]\nThe chat client instance to use for bookmarking. This can be a Chat model provider from chatlas, or more generally, an instance following the ClientWithState protocol.\nrequired\n\n\nbookmark_store\nOptional[BookmarkStore]\nA convenience parameter to set the shiny.express.app_opts(bookmark_store=) which is required for bookmarking (and .enable_bookmarking()). If None, no value will be set.\nNone\n\n\nbookmark_on\nOptional[Literal['response']]\nThe event to trigger the bookmarking on. Supported values include: - \"response\" (the default): a bookmark is triggered when the assistant is done responding. - None: no bookmark is triggered When this method triggers a bookmark, it also updates the URL query string to reflect the bookmarked state.\n'response'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the Shiny App does have bookmarking enabled.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nCancelCallback\nA callback to cancel the bookmarking hooks.\n\n\n\n\n\n\n\nexpress.Chat.message_stream_context()\n    Message stream context manager.\n\n    A context manager for appending streaming messages into the chat. This context\n    manager can:\n\n    1. Be used in isolation to append a new streaming message to the chat.\n        * Compared to `.append_message_stream()` this method is more flexible but\n          isn't non-blocking by default (i.e., it doesn't launch an extended task).\n    2. Be nested within itself\n        * Nesting is primarily useful for making checkpoints to `.clear()` back\n          to (see the example below).\n    3. Be used from within a `.append_message_stream()`\n        * Useful for inserting additional content from another context into the\n          stream (e.g., see the note about tool calls below).\n\n\n    :\n        A `MessageStream` class instance, which has a method for `.append()`ing\n        message content chunks to as well as way to `.clear()` the stream back to\n        it's initial state. Note that `.append()` supports the same message content\n        types as `.append_message()`.\n\n\n\n    ```python\n    import asyncio\n\n    from shiny import reactive\n    from shiny.express import ui\n\n    chat = ui.Chat(id=\"my_chat\")\n    chat.ui()\n\n\n    @reactive.effect\n    async def _():\n        async with chat.message_stream_context() as msg:\n            await msg.append(\"Starting stream...\nProgress:“) async with chat.message_stream_context() as progress: for x in [0, 50, 100]: await progress.append(f” {x}%“) await asyncio.sleep(1) await progress.clear() await msg.clear() await msg.append(”Completed stream”) ```\n\n\n\n    A useful pattern for displaying tool calls in a chatbot is for the tool to\n    display using `.message_stream_context()` while the the response generation is\n    happening through `.append_message_stream()`. This allows the tool to display\n    things like progress updates (or other \"ephemeral\" content) and optionally\n    `.clear()` the stream back to it's initial state when ready to display the\n    \"final\" content.\n\n\n\n\nexpress.Chat.messages(\n    format=MISSING,\n    token_limits=None,\n    transform_user='all',\n    transform_assistant=False,\n)\nReactively read chat messages\nObtain chat messages within a reactive context. The default behavior is intended for passing messages along to a model for response generation where you typically want to:\n\nCap the number of tokens sent in a single request (i.e., token_limits).\nApply user input transformations (i.e., transform_user), if any.\nNot apply assistant response transformations (i.e., transform_assistant) since these are predominantly for display purposes (i.e., the model shouldn’t concern itself with how the responses are displayed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat\nMISSING_TYPE | ProviderMessageFormat\nThe message format to return. The default value of MISSING means chat messages are returned as :class:ChatMessage objects (a dictionary with content and role keys). Other supported formats include: * \"anthropic\": Anthropic message format. * \"google\": Google message (aka content) format. * \"langchain\": LangChain message format. * \"openai\": OpenAI message format. * \"ollama\": Ollama message format.\nMISSING\n\n\ntoken_limits\ntuple[int, int] | None\nLimit the conversation history based on token limits. If specified, only the most recent messages that fit within the token limits are returned. This is useful for avoiding “exceeded token limit” errors when sending messages to the relevant model, while still providing the most recent context available. A specified value must be a tuple of two integers. The first integer is the maximum number of tokens that can be sent to the model in a single request. The second integer is the amount of tokens to reserve for the model’s response. Note that token counts based on the tokenizer provided to the Chat constructor.\nNone\n\n\ntransform_user\nLiteral['all', 'last', 'none']\nWhether to return user input messages with transformation applied. This only matters if a transform_user_input was provided to the chat constructor. The default value of \"all\" means all user input messages are transformed. The value of \"last\" means only the last user input message is transformed. The value of \"none\" means no user input messages are transformed.\n'all'\n\n\ntransform_assistant\nbool\nWhether to return assistant messages with transformation applied. This only matters if an transform_assistant_response was provided to the chat constructor.\nFalse\n\n\n\n\n\n\nMessages are listed in the order they were added. As a result, when this method is called in a .on_user_submit() callback (as it most often is), the last message will be the most recent one submitted by the user.\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[ChatMessage, …]\nA tuple of chat messages.\n\n\n\n\n\n\n\nexpress.Chat.on_user_submit(fn=None)\nDefine a function to invoke when user input is submitted.\nApply this method as a decorator to a function (fn) that should be invoked when the user submits a message. This function can take an optional argument, which will be the user input message.\nIn many cases, the implementation of fn should also do the following:\n\nGenerate a response based on the user input.\n\n\nIf the response should be aware of chat history, use a package like chatlas to manage the chat state, or use the .messages() method to get the chat history.\n\n\nAppend that response to the chat component using .append_message() ( or .append_message_stream() if the response is streamed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nUserSubmitFunction | None\nA function to invoke when user input is submitted.\nNone\n\n\n\n\n\n\nThis method creates a reactive effect that only gets invalidated when the user submits a message. Thus, the function fn can read other reactive dependencies, but it will only be re-invoked when the user submits a message.\n\n\n\n\nexpress.Chat.set_user_message(value)\nDeprecated. Use update_user_input(value=value) instead.\n\n\n\nexpress.Chat.transform_assistant_response(fn=None)\nTransform assistant responses.\nUse this method as a decorator on a function (fn) that transforms assistant responses before displaying them in the chat. This is useful for post-processing model responses before displaying them to the user.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformAssistantResponseFunction | None\nA function that takes a string and returns either a string, :class:shiny.ui.HTML, or None. If fn returns a string, it gets interpreted and parsed as a markdown on the client (and the resulting HTML is then sanitized). If fn returns :class:shiny.ui.HTML, it will be displayed as-is. If fn returns None, the response is effectively ignored.\nNone\n\n\n\n\n\n\nWhen doing an .append_message_stream(), fn gets called on every chunk of the response (thus, it should be performant), and can optionally access more information (i.e., arguments) about the stream. The 1st argument (required) contains the accumulated content, the 2nd argument (optional) contains the current chunk, and the 3rd argument (optional) is a boolean indicating whether this chunk is the last one in the stream.\n\n\n\n\nexpress.Chat.transform_user_input(fn=None)\nTransform user input.\nUse this method as a decorator on a function (fn) that transforms user input before storing it in the chat messages returned by .messages(). This is useful for implementing RAG workflows, like taking a URL and scraping it for text before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformUserInput | TransformUserInputAsync | None\nA function to transform user input before storing it in the chat .messages(). If fn returns None, the user input is effectively ignored, and .on_user_submit() callbacks are suspended until more input is submitted. This behavior is often useful to catch and handle errors that occur during transformation. In this case, the transform function should append an error message to the chat (via .append_message()) to inform the user of the error.\nNone\n\n\n\n\n\n\n\nexpress.Chat.ui(\n    messages=None,\n    placeholder='Enter a message...',\n    width='min(680px, 100%)',\n    height='auto',\n    fill=True,\n    icon_assistant=None,\n    **kwargs,\n)\nCreate a UI element for this Chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessages\nOptional[Sequence[TagChild | ChatMessageDict]]\nA sequence of messages to display in the chat. Each message can be either a string or a dictionary with content and role keys. The content key should contain the message text, and the role key can be “assistant” or “user”.\nNone\n\n\nplaceholder\nstr\nPlaceholder text for the chat input.\n'Enter a message...'\n\n\nwidth\nCssUnit\nThe width of the UI element.\n'min(680px, 100%)'\n\n\nheight\nCssUnit\nThe height of the UI element.\n'auto'\n\n\nfill\nbool\nWhether the chat should vertically take available space inside a fillable container.\nTrue\n\n\nicon_assistant\nHTML | Tag | TagList | None\nThe icon to use for the assistant chat messages. Can be a HTML or a tag in the form of :class:~htmltools.HTML or :class:~htmltools.Tag. If None, a default robot icon is used.\nNone\n\n\nkwargs\nTagAttrValue\nAdditional attributes for the chat container element.\n{}\n\n\n\n\n\n\n\nexpress.Chat.update_user_input(\n    value=None,\n    placeholder=None,\n    submit=False,\n    focus=False,\n)\nUpdate the user input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nstr | None\nThe value to set the user input to.\nNone\n\n\nplaceholder\nstr | None\nThe placeholder text for the user input.\nNone\n\n\nsubmit\nbool\nWhether to automatically submit the text for the user. Requires value.\nFalse\n\n\nfocus\nbool\nWhether to move focus to the input element. Requires value.\nFalse\n\n\n\n\n\n\n\nexpress.Chat.user_input(transform=False)\nReactively read the user’s message.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntransform\nbool\nWhether to apply the user input transformation function (if one was provided).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr | None\nThe user input message (before any transformation).\n\n\n\n\n\n\nMost users shouldn’t need to use this method directly since the last item in .messages() contains the most recent user input. It can be useful for:\n\nTaking a reactive dependency on the user’s input outside of a .on_user_submit() callback.\nMaintaining message state separately from .messages().",
    "crumbs": [
      "API Reference",
      "Shiny Express",
      "express.Chat"
    ]
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "shinychat Reference",
    "section": "",
    "text": "Chat\nCreate a chat interface.\n\n\nchat_ui\nUI container for a chat component (Shiny Core).\n\n\n\n\n\n\n\n\n\nexpress.Chat\n\n\n\n\n\n\n\n\n\n\nplaywright.ChatController\nController for :func:shiny.ui.chat.",
    "crumbs": [
      "API Reference",
      "shinychat Reference"
    ]
  },
  {
    "objectID": "api/index.html#shiny-core",
    "href": "api/index.html#shiny-core",
    "title": "shinychat Reference",
    "section": "",
    "text": "Chat\nCreate a chat interface.\n\n\nchat_ui\nUI container for a chat component (Shiny Core).",
    "crumbs": [
      "API Reference",
      "shinychat Reference"
    ]
  },
  {
    "objectID": "api/index.html#shiny-express",
    "href": "api/index.html#shiny-express",
    "title": "shinychat Reference",
    "section": "",
    "text": "express.Chat",
    "crumbs": [
      "API Reference",
      "shinychat Reference"
    ]
  },
  {
    "objectID": "api/index.html#testing",
    "href": "api/index.html#testing",
    "title": "shinychat Reference",
    "section": "",
    "text": "playwright.ChatController\nController for :func:shiny.ui.chat.",
    "crumbs": [
      "API Reference",
      "shinychat Reference"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "shinychat",
    "section": "",
    "text": "Chat UI component for Shiny for Python."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "shinychat",
    "section": "Installation",
    "text": "Installation\nYou can install shinychat from PyPI with:\nuv pip install shinychat\nOr, install the development version of shinychat from GitHub with:\nuv pip install git+https://github.com/posit-dev/shinychat.git"
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "shinychat",
    "section": "Example",
    "text": "Example\nTo run this example, you’ll first need to create an OpenAI API key, and set it in your environment as OPENAI_API_KEY.\nfrom shiny.express import render, ui\nfrom shinychat.express import Chat\n\n# Set some Shiny page options\nui.page_opts(title=\"Hello Chat\")\n\n# Create a chat instance, with an initial message\nchat = Chat(\n    id=\"chat\",\n    messages=[\n        {\"content\": \"Hello! How can I help you today?\", \"role\": \"assistant\"},\n    ],\n)\n\n# Display the chat\nchat.ui()\n\n\n# Define a callback to run when the user submits a message\n@chat.on_user_submit\nasync def handle_user_input(user_input: str):\n    await chat.append_message(f\"You said: {user_input}\")\n\n\n\"Message state:\"\n\n\n@render.code\ndef message_state():\n    return str(chat.messages())"
  },
  {
    "objectID": "api/chat_ui.html",
    "href": "api/chat_ui.html",
    "title": "chat_ui",
    "section": "",
    "text": "chat_ui(\n    id,\n    *,\n    messages=None,\n    placeholder='Enter a message...',\n    width='min(680px, 100%)',\n    height='auto',\n    fill=True,\n    icon_assistant=None,\n    **kwargs,\n)\nUI container for a chat component (Shiny Core).\nThis function is for locating a :class:~shiny.ui.Chat instance in a Shiny Core app. If you are using Shiny Express, use the :method:~shiny.ui.Chat.ui method instead.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid\nstr\nA unique identifier for the chat UI.\nrequired\n\n\nmessages\nOptional[Sequence[TagChild | ChatMessageDict]]\nA sequence of messages to display in the chat. A given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain a content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see :method:~shiny.ui.Chat.append_message for more info).\nNone\n\n\nplaceholder\nstr\nPlaceholder text for the chat input.\n'Enter a message...'\n\n\nwidth\nCssUnit\nThe width of the chat container.\n'min(680px, 100%)'\n\n\nheight\nCssUnit\nThe height of the chat container.\n'auto'\n\n\nfill\nbool\nWhether the chat should vertically take available space inside a fillable container.\nTrue\n\n\nicon_assistant\nHTML | Tag | TagList | None\nThe icon to use for the assistant chat messages. Can be a HTML or a tag in the form of :class:~htmltools.HTML or :class:~htmltools.Tag. If None, a default robot icon is used.\nNone\n\n\nkwargs\nTagAttrValue\nAdditional attributes for the chat container element.\n{}",
    "crumbs": [
      "API Reference",
      "Shiny Core",
      "chat_ui"
    ]
  },
  {
    "objectID": "api/chat_ui.html#parameters",
    "href": "api/chat_ui.html#parameters",
    "title": "chat_ui",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nid\nstr\nA unique identifier for the chat UI.\nrequired\n\n\nmessages\nOptional[Sequence[TagChild | ChatMessageDict]]\nA sequence of messages to display in the chat. A given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain a content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see :method:~shiny.ui.Chat.append_message for more info).\nNone\n\n\nplaceholder\nstr\nPlaceholder text for the chat input.\n'Enter a message...'\n\n\nwidth\nCssUnit\nThe width of the chat container.\n'min(680px, 100%)'\n\n\nheight\nCssUnit\nThe height of the chat container.\n'auto'\n\n\nfill\nbool\nWhether the chat should vertically take available space inside a fillable container.\nTrue\n\n\nicon_assistant\nHTML | Tag | TagList | None\nThe icon to use for the assistant chat messages. Can be a HTML or a tag in the form of :class:~htmltools.HTML or :class:~htmltools.Tag. If None, a default robot icon is used.\nNone\n\n\nkwargs\nTagAttrValue\nAdditional attributes for the chat container element.\n{}",
    "crumbs": [
      "API Reference",
      "Shiny Core",
      "chat_ui"
    ]
  },
  {
    "objectID": "api/Chat.html",
    "href": "api/Chat.html",
    "title": "Chat",
    "section": "",
    "text": "Chat(id, *, messages=(), on_error='auto', tokenizer=None)\nCreate a chat interface.\nA UI component for building conversational interfaces. With it, end users can submit messages, which will cause a .on_user_submit() callback to run. That callback gets passed the user input message, which can be used to generate a response. The response can then be appended to the chat using .append_message() or .append_message_stream().\nHere’s a rough outline for how to implement a Chat:\nfrom shiny.express import ui\n\n# Create and display chat instance\nchat = ui.Chat(id=\"my_chat\")\nchat.ui()\n\n\n# Define a callback to run when the user submits a message\n@chat.on_user_submit\nasync def handle_user_input(user_input: str):\n    # Create a response message stream\n    response = await my_model.generate_response(user_input, stream=True)\n    # Append the response into the chat\n    await chat.append_message_stream(response)\nIn the outline above, my_model.generate_response() is a placeholder for the function that generates a response based on the chat’s messages. This function will look different depending on the model you’re using, but it will generally involve passing the messages to the model and getting a response back. Also, you’ll typically have a choice to stream=True the response generation, and in that case, you’ll use .append_message_stream() instead of .append_message() to append the response to the chat. Streaming is preferrable when available since it allows for more responsive and scalable chat interfaces.\nIt is also highly recommended to use a package like chatlas to generate responses, especially when responses should be aware of the chat history, support tool calls, etc. See this article to learn more.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid\nstr\nA unique identifier for the chat session. In Shiny Core, make sure this id matches a corresponding :func:~shiny.ui.chat_ui call in the UI.\nrequired\n\n\nmessages\nSequence[Any]\nA sequence of messages to display in the chat. A given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain a content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see .append_message() for more information).\n()\n\n\non_error\nLiteral['auto', 'actual', 'sanitize', 'unhandled']\nHow to handle errors that occur in response to user input. When \"unhandled\", the app will stop running when an error occurs. Otherwise, a notification is displayed to the user and the app continues to run. * \"auto\": Sanitize the error message if the app is set to sanitize errors, otherwise display the actual error message. * \"actual\": Display the actual error message to the user. * \"sanitize\": Sanitize the error message before displaying it to the user. * \"unhandled\": Do not display any error message to the user.\n'auto'\n\n\ntokenizer\nTokenEncoding | None\nThe tokenizer to use for calculating token counts, which is required to impose token_limits in .messages(). If not provided, a default generic tokenizer is attempted to be loaded from the tokenizers library. A specific tokenizer may also be provided by following the TokenEncoding (tiktoken or tozenizers) protocol (e.g., tiktoken.encoding_for_model(\"gpt-4o\")).\nNone\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nlatest_message_stream\nReact to changes in the latest message stream.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nappend_message\nAppend a message to the chat.\n\n\nappend_message_stream\nAppend a message as a stream of message chunks.\n\n\nclear_messages\nClear all chat messages.\n\n\ndestroy\nDestroy the chat instance.\n\n\nenable_bookmarking\nEnable bookmarking for the chat instance.\n\n\nmessage_stream_context\nMessage stream context manager.\n\n\nmessages\nReactively read chat messages\n\n\non_user_submit\nDefine a function to invoke when user input is submitted.\n\n\nset_user_message\nDeprecated. Use update_user_input(value=value) instead.\n\n\ntransform_assistant_response\nTransform assistant responses.\n\n\ntransform_user_input\nTransform user input.\n\n\nupdate_user_input\nUpdate the user input.\n\n\nuser_input\nReactively read the user’s message.\n\n\n\n\n\nChat.append_message(message, *, icon=None)\nAppend a message to the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nAny\nA given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | TagList | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nInput suggestions\n\n\n\nInput suggestions are special links that send text to the user input box when clicked (or accessed via keyboard). They can be created in the following ways:\n\n&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;: An image link with the same functionality as above.\n&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\nAdding a submit CSS class or a data-suggestion-submit=\"true\" attribute to the suggestion element.\nHolding the Ctrl/Cmd key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the Alt/Option key while clicking the suggestion link.\n\n\n\n\n\n\n\n\nStreamed messages\n\n\n\nUse .append_message_stream() instead of this method when stream=True (or similar) is specified in model’s completion method.\n\n\n\n\n\n\nChat.append_message_stream(message, *, icon=None)\nAppend a message as a stream of message chunks.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nIterable[Any] | AsyncIterable[Any]\nAn (async) iterable of message chunks. Each chunk can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\nInput suggestions are special links that send text to the user input box when\nclicked (or accessed via keyboard). They can be created in the following ways:\n\n* `&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;`: An inline text link that\n    places 'Suggestion text' in the user input box when clicked.\n* `&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;`: An image link with\n    the same functionality as above.\n* `&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;`: An inline text\n    link that places 'Suggestion text' in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\n* Adding a `submit` CSS class or a `data-suggestion-submit=\"true\"` attribute to\n  the suggestion element.\n* Holding the `Ctrl/Cmd` key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the\n`Alt/Option` key while clicking the suggestion link.\nUse this method (over `.append_message()`) when `stream=True` (or similar) is\nspecified in model's completion method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nAn extended task that represents the streaming task. The .result() method of the task can be called in a reactive context to get the final state of the stream.\n\n\n\n\n\n\n\nChat.clear_messages()\nClear all chat messages.\n\n\n\nChat.destroy()\nDestroy the chat instance.\n\n\n\nChat.enable_bookmarking(client, /, *, bookmark_on='response')\nEnable bookmarking for the chat instance.\nThis method registers on_bookmark and on_restore hooks on session.bookmark (:class:shiny.bookmark.Bookmark) to save/restore chat state on both the Chat and client= instances. In order for this method to actually work correctly, a bookmark_store= must be specified in shiny.App().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nclient\nClientWithState | chatlas.Chat[Any, Any]\nThe chat client instance to use for bookmarking. This can be a Chat model provider from chatlas, or more generally, an instance following the ClientWithState protocol.\nrequired\n\n\nbookmark_on\nOptional[Literal['response']]\nThe event to trigger the bookmarking on. Supported values include: - \"response\" (the default): a bookmark is triggered when the assistant is done responding. - None: no bookmark is triggered When this method triggers a bookmark, it also updates the URL query string to reflect the bookmarked state.\n'response'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the Shiny App does have bookmarking enabled.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nCancelCallback\nA callback to cancel the bookmarking hooks.\n\n\n\n\n\n\n\nChat.message_stream_context()\n    Message stream context manager.\n\n    A context manager for appending streaming messages into the chat. This context\n    manager can:\n\n    1. Be used in isolation to append a new streaming message to the chat.\n        * Compared to `.append_message_stream()` this method is more flexible but\n          isn't non-blocking by default (i.e., it doesn't launch an extended task).\n    2. Be nested within itself\n        * Nesting is primarily useful for making checkpoints to `.clear()` back\n          to (see the example below).\n    3. Be used from within a `.append_message_stream()`\n        * Useful for inserting additional content from another context into the\n          stream (e.g., see the note about tool calls below).\n\n\n    :\n        A `MessageStream` class instance, which has a method for `.append()`ing\n        message content chunks to as well as way to `.clear()` the stream back to\n        it's initial state. Note that `.append()` supports the same message content\n        types as `.append_message()`.\n\n\n\n    ```python\n    import asyncio\n\n    from shiny import reactive\n    from shiny.express import ui\n\n    chat = ui.Chat(id=\"my_chat\")\n    chat.ui()\n\n\n    @reactive.effect\n    async def _():\n        async with chat.message_stream_context() as msg:\n            await msg.append(\"Starting stream...\nProgress:“) async with chat.message_stream_context() as progress: for x in [0, 50, 100]: await progress.append(f” {x}%“) await asyncio.sleep(1) await progress.clear() await msg.clear() await msg.append(”Completed stream”) ```\n\n\n\n    A useful pattern for displaying tool calls in a chatbot is for the tool to\n    display using `.message_stream_context()` while the the response generation is\n    happening through `.append_message_stream()`. This allows the tool to display\n    things like progress updates (or other \"ephemeral\" content) and optionally\n    `.clear()` the stream back to it's initial state when ready to display the\n    \"final\" content.\n\n\n\n\nChat.messages(\n    format=MISSING,\n    token_limits=None,\n    transform_user='all',\n    transform_assistant=False,\n)\nReactively read chat messages\nObtain chat messages within a reactive context. The default behavior is intended for passing messages along to a model for response generation where you typically want to:\n\nCap the number of tokens sent in a single request (i.e., token_limits).\nApply user input transformations (i.e., transform_user), if any.\nNot apply assistant response transformations (i.e., transform_assistant) since these are predominantly for display purposes (i.e., the model shouldn’t concern itself with how the responses are displayed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat\nMISSING_TYPE | ProviderMessageFormat\nThe message format to return. The default value of MISSING means chat messages are returned as :class:ChatMessage objects (a dictionary with content and role keys). Other supported formats include: * \"anthropic\": Anthropic message format. * \"google\": Google message (aka content) format. * \"langchain\": LangChain message format. * \"openai\": OpenAI message format. * \"ollama\": Ollama message format.\nMISSING\n\n\ntoken_limits\ntuple[int, int] | None\nLimit the conversation history based on token limits. If specified, only the most recent messages that fit within the token limits are returned. This is useful for avoiding “exceeded token limit” errors when sending messages to the relevant model, while still providing the most recent context available. A specified value must be a tuple of two integers. The first integer is the maximum number of tokens that can be sent to the model in a single request. The second integer is the amount of tokens to reserve for the model’s response. Note that token counts based on the tokenizer provided to the Chat constructor.\nNone\n\n\ntransform_user\nLiteral['all', 'last', 'none']\nWhether to return user input messages with transformation applied. This only matters if a transform_user_input was provided to the chat constructor. The default value of \"all\" means all user input messages are transformed. The value of \"last\" means only the last user input message is transformed. The value of \"none\" means no user input messages are transformed.\n'all'\n\n\ntransform_assistant\nbool\nWhether to return assistant messages with transformation applied. This only matters if an transform_assistant_response was provided to the chat constructor.\nFalse\n\n\n\n\n\n\nMessages are listed in the order they were added. As a result, when this method is called in a .on_user_submit() callback (as it most often is), the last message will be the most recent one submitted by the user.\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[ChatMessage, …]\nA tuple of chat messages.\n\n\n\n\n\n\n\nChat.on_user_submit(fn=None)\nDefine a function to invoke when user input is submitted.\nApply this method as a decorator to a function (fn) that should be invoked when the user submits a message. This function can take an optional argument, which will be the user input message.\nIn many cases, the implementation of fn should also do the following:\n\nGenerate a response based on the user input.\n\n\nIf the response should be aware of chat history, use a package like chatlas to manage the chat state, or use the .messages() method to get the chat history.\n\n\nAppend that response to the chat component using .append_message() ( or .append_message_stream() if the response is streamed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nUserSubmitFunction | None\nA function to invoke when user input is submitted.\nNone\n\n\n\n\n\n\nThis method creates a reactive effect that only gets invalidated when the user submits a message. Thus, the function fn can read other reactive dependencies, but it will only be re-invoked when the user submits a message.\n\n\n\n\nChat.set_user_message(value)\nDeprecated. Use update_user_input(value=value) instead.\n\n\n\nChat.transform_assistant_response(fn=None)\nTransform assistant responses.\nUse this method as a decorator on a function (fn) that transforms assistant responses before displaying them in the chat. This is useful for post-processing model responses before displaying them to the user.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformAssistantResponseFunction | None\nA function that takes a string and returns either a string, :class:shiny.ui.HTML, or None. If fn returns a string, it gets interpreted and parsed as a markdown on the client (and the resulting HTML is then sanitized). If fn returns :class:shiny.ui.HTML, it will be displayed as-is. If fn returns None, the response is effectively ignored.\nNone\n\n\n\n\n\n\nWhen doing an .append_message_stream(), fn gets called on every chunk of the response (thus, it should be performant), and can optionally access more information (i.e., arguments) about the stream. The 1st argument (required) contains the accumulated content, the 2nd argument (optional) contains the current chunk, and the 3rd argument (optional) is a boolean indicating whether this chunk is the last one in the stream.\n\n\n\n\nChat.transform_user_input(fn=None)\nTransform user input.\nUse this method as a decorator on a function (fn) that transforms user input before storing it in the chat messages returned by .messages(). This is useful for implementing RAG workflows, like taking a URL and scraping it for text before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformUserInput | TransformUserInputAsync | None\nA function to transform user input before storing it in the chat .messages(). If fn returns None, the user input is effectively ignored, and .on_user_submit() callbacks are suspended until more input is submitted. This behavior is often useful to catch and handle errors that occur during transformation. In this case, the transform function should append an error message to the chat (via .append_message()) to inform the user of the error.\nNone\n\n\n\n\n\n\n\nChat.update_user_input(value=None, placeholder=None, submit=False, focus=False)\nUpdate the user input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nstr | None\nThe value to set the user input to.\nNone\n\n\nplaceholder\nstr | None\nThe placeholder text for the user input.\nNone\n\n\nsubmit\nbool\nWhether to automatically submit the text for the user. Requires value.\nFalse\n\n\nfocus\nbool\nWhether to move focus to the input element. Requires value.\nFalse\n\n\n\n\n\n\n\nChat.user_input(transform=False)\nReactively read the user’s message.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntransform\nbool\nWhether to apply the user input transformation function (if one was provided).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr | None\nThe user input message (before any transformation).\n\n\n\n\n\n\nMost users shouldn’t need to use this method directly since the last item in .messages() contains the most recent user input. It can be useful for:\n\nTaking a reactive dependency on the user’s input outside of a .on_user_submit() callback.\nMaintaining message state separately from .messages().",
    "crumbs": [
      "API Reference",
      "Shiny Core",
      "Chat"
    ]
  },
  {
    "objectID": "api/Chat.html#parameters",
    "href": "api/Chat.html#parameters",
    "title": "Chat",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nid\nstr\nA unique identifier for the chat session. In Shiny Core, make sure this id matches a corresponding :func:~shiny.ui.chat_ui call in the UI.\nrequired\n\n\nmessages\nSequence[Any]\nA sequence of messages to display in the chat. A given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain a content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see .append_message() for more information).\n()\n\n\non_error\nLiteral['auto', 'actual', 'sanitize', 'unhandled']\nHow to handle errors that occur in response to user input. When \"unhandled\", the app will stop running when an error occurs. Otherwise, a notification is displayed to the user and the app continues to run. * \"auto\": Sanitize the error message if the app is set to sanitize errors, otherwise display the actual error message. * \"actual\": Display the actual error message to the user. * \"sanitize\": Sanitize the error message before displaying it to the user. * \"unhandled\": Do not display any error message to the user.\n'auto'\n\n\ntokenizer\nTokenEncoding | None\nThe tokenizer to use for calculating token counts, which is required to impose token_limits in .messages(). If not provided, a default generic tokenizer is attempted to be loaded from the tokenizers library. A specific tokenizer may also be provided by following the TokenEncoding (tiktoken or tozenizers) protocol (e.g., tiktoken.encoding_for_model(\"gpt-4o\")).\nNone",
    "crumbs": [
      "API Reference",
      "Shiny Core",
      "Chat"
    ]
  },
  {
    "objectID": "api/Chat.html#attributes",
    "href": "api/Chat.html#attributes",
    "title": "Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nlatest_message_stream\nReact to changes in the latest message stream.",
    "crumbs": [
      "API Reference",
      "Shiny Core",
      "Chat"
    ]
  },
  {
    "objectID": "api/Chat.html#methods",
    "href": "api/Chat.html#methods",
    "title": "Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nappend_message\nAppend a message to the chat.\n\n\nappend_message_stream\nAppend a message as a stream of message chunks.\n\n\nclear_messages\nClear all chat messages.\n\n\ndestroy\nDestroy the chat instance.\n\n\nenable_bookmarking\nEnable bookmarking for the chat instance.\n\n\nmessage_stream_context\nMessage stream context manager.\n\n\nmessages\nReactively read chat messages\n\n\non_user_submit\nDefine a function to invoke when user input is submitted.\n\n\nset_user_message\nDeprecated. Use update_user_input(value=value) instead.\n\n\ntransform_assistant_response\nTransform assistant responses.\n\n\ntransform_user_input\nTransform user input.\n\n\nupdate_user_input\nUpdate the user input.\n\n\nuser_input\nReactively read the user’s message.\n\n\n\n\n\nChat.append_message(message, *, icon=None)\nAppend a message to the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nAny\nA given message can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | TagList | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nInput suggestions\n\n\n\nInput suggestions are special links that send text to the user input box when clicked (or accessed via keyboard). They can be created in the following ways:\n\n&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;: An image link with the same functionality as above.\n&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;: An inline text link that places ‘Suggestion text’ in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\nAdding a submit CSS class or a data-suggestion-submit=\"true\" attribute to the suggestion element.\nHolding the Ctrl/Cmd key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the Alt/Option key while clicking the suggestion link.\n\n\n\n\n\n\n\n\nStreamed messages\n\n\n\nUse .append_message_stream() instead of this method when stream=True (or similar) is specified in model’s completion method.\n\n\n\n\n\n\nChat.append_message_stream(message, *, icon=None)\nAppend a message as a stream of message chunks.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmessage\nIterable[Any] | AsyncIterable[Any]\nAn (async) iterable of message chunks. Each chunk can be one of the following: * A string, which is interpreted as markdown and rendered to HTML on the client. * To prevent interpreting as markdown, mark the string as :class:~shiny.ui.HTML. * A UI element (specifically, a :class:~shiny.ui.TagChild). * This includes :class:~shiny.ui.TagList, which take UI elements (including strings) as children. In this case, strings are still interpreted as markdown as long as they’re not inside HTML. * A dictionary with content and role keys. The content key can contain content as described above, and the role key can be “assistant” or “user”. NOTE: content may include specially formatted input suggestion links (see note below).\nrequired\n\n\nicon\nHTML | Tag | None\nAn optional icon to display next to the message, currently only used for assistant messages. The icon can be any HTML element (e.g., an :func:~shiny.ui.img tag) or a string of HTML.\nNone\n\n\n\n\n\n\nInput suggestions are special links that send text to the user input box when\nclicked (or accessed via keyboard). They can be created in the following ways:\n\n* `&lt;span class='suggestion'&gt;Suggestion text&lt;/span&gt;`: An inline text link that\n    places 'Suggestion text' in the user input box when clicked.\n* `&lt;img data-suggestion='Suggestion text' src='image.jpg'&gt;`: An image link with\n    the same functionality as above.\n* `&lt;span data-suggestion='Suggestion text'&gt;Actual text&lt;/span&gt;`: An inline text\n    link that places 'Suggestion text' in the user input box when clicked.\n\nA suggestion can also be submitted automatically by doing one of the following:\n\n* Adding a `submit` CSS class or a `data-suggestion-submit=\"true\"` attribute to\n  the suggestion element.\n* Holding the `Ctrl/Cmd` key while clicking the suggestion link.\n\nNote that a user may also opt-out of submitting a suggestion by holding the\n`Alt/Option` key while clicking the suggestion link.\nUse this method (over `.append_message()`) when `stream=True` (or similar) is\nspecified in model's completion method.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nAn extended task that represents the streaming task. The .result() method of the task can be called in a reactive context to get the final state of the stream.\n\n\n\n\n\n\n\nChat.clear_messages()\nClear all chat messages.\n\n\n\nChat.destroy()\nDestroy the chat instance.\n\n\n\nChat.enable_bookmarking(client, /, *, bookmark_on='response')\nEnable bookmarking for the chat instance.\nThis method registers on_bookmark and on_restore hooks on session.bookmark (:class:shiny.bookmark.Bookmark) to save/restore chat state on both the Chat and client= instances. In order for this method to actually work correctly, a bookmark_store= must be specified in shiny.App().\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nclient\nClientWithState | chatlas.Chat[Any, Any]\nThe chat client instance to use for bookmarking. This can be a Chat model provider from chatlas, or more generally, an instance following the ClientWithState protocol.\nrequired\n\n\nbookmark_on\nOptional[Literal['response']]\nThe event to trigger the bookmarking on. Supported values include: - \"response\" (the default): a bookmark is triggered when the assistant is done responding. - None: no bookmark is triggered When this method triggers a bookmark, it also updates the URL query string to reflect the bookmarked state.\n'response'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the Shiny App does have bookmarking enabled.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nCancelCallback\nA callback to cancel the bookmarking hooks.\n\n\n\n\n\n\n\nChat.message_stream_context()\n    Message stream context manager.\n\n    A context manager for appending streaming messages into the chat. This context\n    manager can:\n\n    1. Be used in isolation to append a new streaming message to the chat.\n        * Compared to `.append_message_stream()` this method is more flexible but\n          isn't non-blocking by default (i.e., it doesn't launch an extended task).\n    2. Be nested within itself\n        * Nesting is primarily useful for making checkpoints to `.clear()` back\n          to (see the example below).\n    3. Be used from within a `.append_message_stream()`\n        * Useful for inserting additional content from another context into the\n          stream (e.g., see the note about tool calls below).\n\n\n    :\n        A `MessageStream` class instance, which has a method for `.append()`ing\n        message content chunks to as well as way to `.clear()` the stream back to\n        it's initial state. Note that `.append()` supports the same message content\n        types as `.append_message()`.\n\n\n\n    ```python\n    import asyncio\n\n    from shiny import reactive\n    from shiny.express import ui\n\n    chat = ui.Chat(id=\"my_chat\")\n    chat.ui()\n\n\n    @reactive.effect\n    async def _():\n        async with chat.message_stream_context() as msg:\n            await msg.append(\"Starting stream...\nProgress:“) async with chat.message_stream_context() as progress: for x in [0, 50, 100]: await progress.append(f” {x}%“) await asyncio.sleep(1) await progress.clear() await msg.clear() await msg.append(”Completed stream”) ```\n\n\n\n    A useful pattern for displaying tool calls in a chatbot is for the tool to\n    display using `.message_stream_context()` while the the response generation is\n    happening through `.append_message_stream()`. This allows the tool to display\n    things like progress updates (or other \"ephemeral\" content) and optionally\n    `.clear()` the stream back to it's initial state when ready to display the\n    \"final\" content.\n\n\n\n\nChat.messages(\n    format=MISSING,\n    token_limits=None,\n    transform_user='all',\n    transform_assistant=False,\n)\nReactively read chat messages\nObtain chat messages within a reactive context. The default behavior is intended for passing messages along to a model for response generation where you typically want to:\n\nCap the number of tokens sent in a single request (i.e., token_limits).\nApply user input transformations (i.e., transform_user), if any.\nNot apply assistant response transformations (i.e., transform_assistant) since these are predominantly for display purposes (i.e., the model shouldn’t concern itself with how the responses are displayed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat\nMISSING_TYPE | ProviderMessageFormat\nThe message format to return. The default value of MISSING means chat messages are returned as :class:ChatMessage objects (a dictionary with content and role keys). Other supported formats include: * \"anthropic\": Anthropic message format. * \"google\": Google message (aka content) format. * \"langchain\": LangChain message format. * \"openai\": OpenAI message format. * \"ollama\": Ollama message format.\nMISSING\n\n\ntoken_limits\ntuple[int, int] | None\nLimit the conversation history based on token limits. If specified, only the most recent messages that fit within the token limits are returned. This is useful for avoiding “exceeded token limit” errors when sending messages to the relevant model, while still providing the most recent context available. A specified value must be a tuple of two integers. The first integer is the maximum number of tokens that can be sent to the model in a single request. The second integer is the amount of tokens to reserve for the model’s response. Note that token counts based on the tokenizer provided to the Chat constructor.\nNone\n\n\ntransform_user\nLiteral['all', 'last', 'none']\nWhether to return user input messages with transformation applied. This only matters if a transform_user_input was provided to the chat constructor. The default value of \"all\" means all user input messages are transformed. The value of \"last\" means only the last user input message is transformed. The value of \"none\" means no user input messages are transformed.\n'all'\n\n\ntransform_assistant\nbool\nWhether to return assistant messages with transformation applied. This only matters if an transform_assistant_response was provided to the chat constructor.\nFalse\n\n\n\n\n\n\nMessages are listed in the order they were added. As a result, when this method is called in a .on_user_submit() callback (as it most often is), the last message will be the most recent one submitted by the user.\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple[ChatMessage, …]\nA tuple of chat messages.\n\n\n\n\n\n\n\nChat.on_user_submit(fn=None)\nDefine a function to invoke when user input is submitted.\nApply this method as a decorator to a function (fn) that should be invoked when the user submits a message. This function can take an optional argument, which will be the user input message.\nIn many cases, the implementation of fn should also do the following:\n\nGenerate a response based on the user input.\n\n\nIf the response should be aware of chat history, use a package like chatlas to manage the chat state, or use the .messages() method to get the chat history.\n\n\nAppend that response to the chat component using .append_message() ( or .append_message_stream() if the response is streamed).\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nUserSubmitFunction | None\nA function to invoke when user input is submitted.\nNone\n\n\n\n\n\n\nThis method creates a reactive effect that only gets invalidated when the user submits a message. Thus, the function fn can read other reactive dependencies, but it will only be re-invoked when the user submits a message.\n\n\n\n\nChat.set_user_message(value)\nDeprecated. Use update_user_input(value=value) instead.\n\n\n\nChat.transform_assistant_response(fn=None)\nTransform assistant responses.\nUse this method as a decorator on a function (fn) that transforms assistant responses before displaying them in the chat. This is useful for post-processing model responses before displaying them to the user.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformAssistantResponseFunction | None\nA function that takes a string and returns either a string, :class:shiny.ui.HTML, or None. If fn returns a string, it gets interpreted and parsed as a markdown on the client (and the resulting HTML is then sanitized). If fn returns :class:shiny.ui.HTML, it will be displayed as-is. If fn returns None, the response is effectively ignored.\nNone\n\n\n\n\n\n\nWhen doing an .append_message_stream(), fn gets called on every chunk of the response (thus, it should be performant), and can optionally access more information (i.e., arguments) about the stream. The 1st argument (required) contains the accumulated content, the 2nd argument (optional) contains the current chunk, and the 3rd argument (optional) is a boolean indicating whether this chunk is the last one in the stream.\n\n\n\n\nChat.transform_user_input(fn=None)\nTransform user input.\nUse this method as a decorator on a function (fn) that transforms user input before storing it in the chat messages returned by .messages(). This is useful for implementing RAG workflows, like taking a URL and scraping it for text before sending it to the model.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\nTransformUserInput | TransformUserInputAsync | None\nA function to transform user input before storing it in the chat .messages(). If fn returns None, the user input is effectively ignored, and .on_user_submit() callbacks are suspended until more input is submitted. This behavior is often useful to catch and handle errors that occur during transformation. In this case, the transform function should append an error message to the chat (via .append_message()) to inform the user of the error.\nNone\n\n\n\n\n\n\n\nChat.update_user_input(value=None, placeholder=None, submit=False, focus=False)\nUpdate the user input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nstr | None\nThe value to set the user input to.\nNone\n\n\nplaceholder\nstr | None\nThe placeholder text for the user input.\nNone\n\n\nsubmit\nbool\nWhether to automatically submit the text for the user. Requires value.\nFalse\n\n\nfocus\nbool\nWhether to move focus to the input element. Requires value.\nFalse\n\n\n\n\n\n\n\nChat.user_input(transform=False)\nReactively read the user’s message.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntransform\nbool\nWhether to apply the user input transformation function (if one was provided).\nFalse\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr | None\nThe user input message (before any transformation).\n\n\n\n\n\n\nMost users shouldn’t need to use this method directly since the last item in .messages() contains the most recent user input. It can be useful for:\n\nTaking a reactive dependency on the user’s input outside of a .on_user_submit() callback.\nMaintaining message state separately from .messages().",
    "crumbs": [
      "API Reference",
      "Shiny Core",
      "Chat"
    ]
  }
]